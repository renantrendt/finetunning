Veja se essas infos contem no file de fine tuning novo

Remember to resize the model's token embeddings when loading:
from transformers import GPT2LMHeadModel, GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('./yanomami_tokenizer/complete_yanomami_tokenizer')
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    model.resize_token_embeddings(len(tokenizer))

#Estratégia de Fine-tuning
Implementar treinamento em múltiplas fases, começando com memorização de definições básicas
Usar técnicas como curriculum learning, começando com exemplos mais simples

#Carregue o modelo já treinado:
model_path = "./gpt2_yanomami_translator"  # Caminho do seu modelo atual
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

#checkpoint salvar
model.save_pretrained("./caminho/para/checkpoint")

#Contagem de Batches e Progresso da Epoch
# Dentro da função train_model()
total_batches = len(train_dataloader)  # Total de batches na epoch

for epoch in range(num_epochs):
    logger.info(f"Iniciando epoch {epoch+1}/{num_epochs}")
    
    for batch_idx, batch in enumerate(train_dataloader):
        # Processar o batch...
        
        # Exibir informações sobre o progresso
        remaining_batches = total_batches - batch_idx - 1
        logger.info(f"Batch {batch_idx + 1}/{total_batches}, Batches restantes: {remaining_batches}")

#Exibir Progresso
progress_percent = (batch_idx + 1) / total_batches * 100
logger.info(f"Progresso da epoch: {progress_percent:.2f}%")

duplique o script, criando um arquivo novo e inclua

- Ajustar a taxa de aprendizado
- Adicionar camadas específicas para tradução
- Técnicas de regularização
- Implementar mixed precision training
- Gradient accumulation
- Melhorar a tokenização
- Implementar early stopping