# Improved Yanomami-English Translation Model Fine-tuning Script
# This script implements a multi-phase training approach for a GPT-2 based
# Yanomami-English translation model with enhanced tokenizer handling for special characters

import os
import json
import logging
import random
import torch
import numpy as np
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from tokenizer_enhancement import enhance_tokenizer, SPECIAL_CHAR_WORDS

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TranslatorConfig:
    """
    Configuration class for the Yanomami-English translator model.
    Contains all parameters for the training process and model configuration.
    """
    def __init__(self):
        # Base model configuration
        self.model_name = "gpt2"
        self.output_dir = "./yanomami_translator_model"
        self.dataset_dir = "./yanomami_dataset"
        
        # Training configuration
        self.device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
        self.batch_size = 4
        self.gradient_accumulation_steps = 8  # Effective batch size = batch_size * gradient_accumulation_steps
        self.mixed_precision = "fp16" if self.device == "cuda" else "no"
        self.max_length = 512
        self.seed = 42
        
        # Multi-phase training configuration
        self.phases = [
            {
                'name': 'Phase 1: Basic vocabulary',
                'dataset_files': ['combined-ok-translations.jsonl'],
                'learning_rate': 5e-5,
                'num_epochs': 5
            },
            {
                'name': 'Phase 2: Grammar and structure',
                'dataset_files': ['grammar-verb.jsonl', 'grammar-plural.jsonl', 'combined-ok-how-to-p1.jsonl'],
                'learning_rate': 3e-5,
                'num_epochs': 8
            },
            {
                'name': 'Phase 3: Advanced phrases and usage',
                'dataset_files': ['combined-ok-how-to-p2.jsonl', 'combined-ok-phrases-english-to-yanomami.jsonl', 'combined-ok-phrases-yanomami-to-english.jsonl', 'comparison.jsonl'],
                'learning_rate': 2e-5,
                'num_epochs': 5
            }
        ]
        
        # Early stopping configuration
        self.early_stopping_patience = 3
        self.evaluation_strategy = "steps"
        self.eval_steps = 500
        self.save_steps = 500
        self.logging_steps = 100
        self.save_total_limit = 3
        
        # Tokenizer enhancement
        self.enhance_tokenizer = True
        self.special_char_words = SPECIAL_CHAR_WORDS

def set_seed(seed):
    """
    Set random seed for reproducibility across different libraries.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def load_jsonl(file_path):
    """
    Load data from a JSONL file.
    
    Args:
        file_path: Path to the JSONL file
        
    Returns:
        List of dictionaries containing the data
    """
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))
        return data
    except Exception as e:
        logger.error(f"Error loading JSONL file {file_path}: {str(e)}")
        return []

def prepare_data_for_training(examples):
    """
    Convert raw data examples into input-output pairs for training.
    
    Args:
        examples: List of dictionaries containing the raw data
        
    Returns:
        List of dictionaries with 'input' and 'output' keys
    """
    processed_data = []
    for example in examples:
        if 'messages' in example and len(example['messages']) >= 2:
            input_text = example['messages'][0]['content']
            output_text = example['messages'][1]['content']
            
            # Format as English to Yanomami by default
            processed_data.append({
                "input": f"English: {input_text} => Yanomami:", 
                "output": output_text
            })
    return processed_data

def tokenize_data(examples, tokenizer, max_length):
    """
    Tokenize the input-output pairs for training.
    
    Args:
        examples: Dictionary containing 'input' and 'output' keys
        tokenizer: The tokenizer to use
        max_length: Maximum sequence length
        
    Returns:
        Dictionary containing the tokenized data
    """
    # Combine input and output for causal language modeling
    texts = [ex["input"] + " " + ex["output"] for ex in examples["data"]]
    
    # Tokenize the texts
    tokenized = tokenizer(
        texts,
        truncation=True,
        max_length=max_length,
        padding="max_length",
        return_tensors="pt"
    )
    
    # Set labels equal to input_ids for causal language modeling
    tokenized["labels"] = tokenized["input_ids"].clone()
    
    return tokenized

def load_and_prepare_dataset(config, phase, tokenizer):
    """
    Load and prepare the dataset for a specific training phase.
    
    Args:
        config: TranslatorConfig instance
        phase: Dictionary containing phase configuration
        tokenizer: The tokenizer to use
        
    Returns:
        Tuple of (train_dataset, eval_dataset)
    """
    logger.info(f"Loading dataset for {phase['name']}")
    
    all_examples = []
    for file_name in phase['dataset_files']:
        file_path = os.path.join(config.dataset_dir, file_name)
        logger.info(f"Loading data from {file_path}")
        
        examples = load_jsonl(file_path)
        logger.info(f"Loaded {len(examples)} examples from {file_name}")
        
        processed_examples = prepare_data_for_training(examples)
        logger.info(f"Processed {len(processed_examples)} examples from {file_name}")
        
        all_examples.extend(processed_examples)
    
    logger.info(f"Total examples for {phase['name']}: {len(all_examples)}")
    
    # Split into train and eval (90/10)
    random.shuffle(all_examples)
    split_idx = int(len(all_examples) * 0.9)
    train_examples = all_examples[:split_idx]
    eval_examples = all_examples[split_idx:]
    
    # Create datasets
    train_dataset = Dataset.from_dict({"data": train_examples})
    eval_dataset = Dataset.from_dict({"data": eval_examples})
    
    # Tokenize datasets
    train_dataset = train_dataset.map(
        lambda examples: tokenize_data(examples, tokenizer, config.max_length),
        batched=True,
        remove_columns=["data"]
    )
    
    eval_dataset = eval_dataset.map(
        lambda examples: tokenize_data(examples, tokenizer, config.max_length),
        batched=True,
        remove_columns=["data"]
    )
    
    return train_dataset, eval_dataset

def train_phase(config, phase, model, tokenizer):
    """
    Train the model for a specific phase.
    
    Args:
        config: TranslatorConfig instance
        phase: Dictionary containing phase configuration
        model: The model to train
        tokenizer: The tokenizer to use
        
    Returns:
        The trained model
    """
    logger.info(f"Starting training for {phase['name']}")
    
    # Load and prepare dataset
    train_dataset, eval_dataset = load_and_prepare_dataset(config, phase, tokenizer)
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # Training arguments
    phase_output_dir = os.path.join(config.output_dir, phase['name'].replace(":", "").replace(" ", "_").lower())
    training_args = TrainingArguments(
        output_dir=phase_output_dir,
        overwrite_output_dir=True,
        num_train_epochs=phase['num_epochs'],
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        learning_rate=phase['learning_rate'],
        weight_decay=0.01,
        warmup_ratio=0.1,
        fp16=(config.mixed_precision == "fp16"),
        evaluation_strategy=config.evaluation_strategy,
        eval_steps=config.eval_steps,
        save_steps=config.save_steps,
        logging_steps=config.logging_steps,
        save_total_limit=config.save_total_limit,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        seed=config.seed
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)]
    )
    
    # Train the model
    trainer.train()
    
    # Save the model
    model.save_pretrained(phase_output_dir)
    tokenizer.save_pretrained(phase_output_dir)
    
    # Evaluate the model
    eval_results = trainer.evaluate()
    logger.info(f"Evaluation results for {phase['name']}: {eval_results}")
    
    return model

def main():
    """
    Main function to run the training process.
    """
    logger.info("Starting Yanomami-English translation model fine-tuning...")
    
    # Initialize configuration
    config = TranslatorConfig()
    
    # Set random seed
    set_seed(config.seed)
    
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    model = AutoModelForCausalLM.from_pretrained(config.model_name)
    
    # Enhance tokenizer with special character handling
    if config.enhance_tokenizer:
        logger.info("Enhancing tokenizer with special character handling")
        tokenizer = enhance_tokenizer(tokenizer)
        
        # Resize model embeddings to match the new tokenizer size
        model.resize_token_embeddings(len(tokenizer))
    
    # Move model to the appropriate device
    model.to(config.device)
    
    # Create output directory if it doesn't exist
    os.makedirs(config.output_dir, exist_ok=True)
    
    # Train the model through multiple phases
    for phase in config.phases:
        model = train_phase(config, phase, model, tokenizer)
        logger.info(f"Completed training for {phase['name']}")
    
    logger.info("Training completed successfully!")

def translate(text, model_path=None, direction="en2yan"):
    """
    Translate text between English and Yanomami.
    
    Args:
        text: The text to translate
        model_path: Path to the trained model (defaults to the latest phase)
        direction: Translation direction, either "en2yan" or "yan2en"
        
    Returns:
        The translated text
    """
    config = TranslatorConfig()
    
    # Determine model path if not provided
    if model_path is None:
        # Use the last phase model by default
        phase_name = config.phases[-1]['name'].replace(":", "").replace(" ", "_").lower()
        model_path = os.path.join(config.output_dir, phase_name)
    
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    model.to(config.device)
    
    # Prepare input based on direction
    if direction == "en2yan":
        input_text = f"English: {text} => Yanomami:"
    else:  # yan2en
        input_text = f"Yanomami: {text} => English:"
    
    # Tokenize input
    inputs = tokenizer(input_text, return_tensors="pt").to(config.device)
    
    # Generate translation
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_length=config.max_length,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode and return the translation
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract only the translated part (after the prompt)
    translation = translation[len(input_text):].strip()
    
    return translation

if __name__ == "__main__":
    main()
